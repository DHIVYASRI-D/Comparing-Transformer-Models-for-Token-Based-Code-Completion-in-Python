{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1KuOrzvy3pKh82IEz7R0G5AIi0NAiyX9W",
      "authorship_tag": "ABX9TyNxIrTdKwYmBgB5IZZIaHG9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DHIVYASRI-D/Comparing-Transformer-Models-for-Token-Based-Code-Completion-in-Python/blob/main/UI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Gradio"
      ],
      "metadata": {
        "id": "8RWP0toXZD-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n"
      ],
      "metadata": {
        "id": "8mT7BPKqZBVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers gradio sentencepiece\n"
      ],
      "metadata": {
        "id": "83nB9ry-eroV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**final output**\n"
      ],
      "metadata": {
        "id": "De_gYY7mWaB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers gradio\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, T5ForConditionalGeneration\n",
        "\n",
        "# Set device to CPU\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Load GPT-Neo 125M\n",
        "neo_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/token-completion-models/gptneo-125M\", local_files_only=True)\n",
        "neo_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/token-completion-models/gptneo-125M\", local_files_only=True).to(device).eval()\n",
        "\n",
        "# Load CodeT5-Base\n",
        "t5_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/fine-tuned-models/codeT5-base-finetuned\", local_files_only=True)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/fine-tuned-models/codeT5-base-finetuned\", local_files_only=True).to(device).eval()\n",
        "\n",
        "# Load CodeGPT-Small-Py\n",
        "codegpt_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/token-completion-models/codegpt-small-py\", local_files_only=True)\n",
        "codegpt_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/token-completion-models/codegpt-small-py\", local_files_only=True).to(device).eval()\n",
        "\n",
        "# Load CodeGen-350M-Multi\n",
        "codegen_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/fine-tuned-models/codegen-350M-multi\", local_files_only=True)\n",
        "codegen_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/fine-tuned-models/codegen-350M-multi\", local_files_only=True).to(device).eval()\n"
      ],
      "metadata": {
        "id": "Aepf8uQTd0zW",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_all_models(prompt, max_tokens):\n",
        "    neo_out = predict_gptneo(prompt, max_tokens)\n",
        "    t5_out = predict_codet5(prompt, max_tokens)\n",
        "    gpt_out = predict_codegpt(prompt, max_tokens)\n",
        "    codegen_out = predict_codegen(prompt, max_tokens)\n",
        "    return neo_out, t5_out, gpt_out, codegen_out\n",
        "\n",
        "def predict_gptneo(prompt, max_tokens):\n",
        "    inputs = neo_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
        "    outputs = neo_model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False, pad_token_id=neo_tokenizer.eos_token_id)\n",
        "    decoded = neo_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded[len(prompt):].strip() if decoded.startswith(prompt) else decoded.strip()\n",
        "\n",
        "def predict_codet5(prompt, max_tokens):\n",
        "    input_ids = t5_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).input_ids.to(\"cpu\")\n",
        "    outputs = t5_model.generate(input_ids, max_new_tokens=max_tokens, do_sample=False, num_beams=1)\n",
        "    return t5_tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "def predict_codegpt(prompt, max_tokens):\n",
        "    inputs = codegpt_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
        "    outputs = codegpt_model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False, pad_token_id=codegpt_tokenizer.eos_token_id)\n",
        "    decoded = codegpt_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded[len(prompt):].strip() if decoded.startswith(prompt) else decoded.strip()\n",
        "\n",
        "def predict_codegen(prompt, max_tokens):\n",
        "    inputs = codegen_tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(\"cpu\")\n",
        "    outputs = codegen_model.generate(**inputs, max_new_tokens=max_tokens, do_sample=False, pad_token_id=codegen_tokenizer.eos_token_id)\n",
        "    decoded = codegen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return decoded[len(prompt):].strip() if decoded.startswith(prompt) else decoded.strip()\n",
        "\n",
        "# UI\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"### ðŸ”® Code Completion with Multiple Models\")\n",
        "\n",
        "    with gr.Row():\n",
        "        prompt = gr.Textbox(label=\"Input Prompt\", placeholder=\"Type your Python code...\")\n",
        "        max_tokens = gr.Slider(1, 10, value=1, step=1, label=\"Tokens to Predict\")\n",
        "\n",
        "    with gr.Row():\n",
        "        btn = gr.Button(\"Generate\")\n",
        "\n",
        "    with gr.Row():\n",
        "        out_neo = gr.Textbox(label=\"GPT-Neo 125M\")\n",
        "        out_t5 = gr.Textbox(label=\"CodeT5-Base\")\n",
        "        out_gpt = gr.Textbox(label=\"CodeGPT-Small-Py\")\n",
        "        out_codegen = gr.Textbox(label=\"CodeGen-350M-Multi\")\n",
        "\n",
        "    btn.click(fn=predict_all_models, inputs=[prompt, max_tokens], outputs=[out_neo, out_t5, out_gpt, out_codegen])\n",
        "\n",
        "demo.launch()\n",
        "\n"
      ],
      "metadata": {
        "id": "Hk4biAkHUEb2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}